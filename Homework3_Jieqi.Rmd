---
title: "Homework 3"
author: "Jieqi Tu"
date: "3/30/2021"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pROC)
```

## 5.3

#### (a)
Construct the ROC curve for the toy example in Section 5.4.2. with complete separation.
```{r 5.3a}
# data import
x = c(1, 2, 3, 4, 5, 6)
y = c(1, 1, 1, 0, 0, 0)

# fit model
toy.model = glm(y~x, family = "binomial")
toy.pred = predict(toy.model, newdata = data.frame(x))
toy.roc = roc(y, toy.pred)
plot(toy.roc, legacy.axes = T, print.auc = T)
summary(toy.model)
```

From the result we could see that, when fitting the GLM, we have a warning message saying that fitted probabilities numerically 0 or 1 occurred. This implies complete/semi-complete separation occurred. 
We could see that the AUC = 1. Therefore, in this case, we have complete separation.The standard error of the estimated coefficient for x is very large.

#### (b)
Add two observations at x = 0.5, one with y = 1 and one with 0.
```{r 5.3b}
x = c(1, 2, 3, 3.5, 3.5, 4, 5, 6)
y = c(1, 1, 1, 1, 0, 0, 0, 0)

toy.model2 = glm(y~x, family = "binomial")
toy.pred = predict(toy.model2, newdata = data.frame(x))
toy.roc = roc(y, toy.pred)
plot(toy.roc, legacy.axes = T, print.auc = T)
summary(toy.model2)
```

In this case, we have AUC = 0.969.But we also have the warning message. So we are facing the semi-complete separation. The standard error for estimated coefficient for x is still very large.

Then we want to construct a toy data set with n = 8 and the area under the ROC curve equals 0.5.
```{r 5.3c}
# construct new dataset
x.new = c(1, 1, 2, 2, 3, 3, 4, 4)
y.new = c(1, 0, 1, 0, 1, 0, 1, 0)

# fit new model for new dataset
toy.model3 = glm(y.new~x.new, family = "binomial")
toy.pred = predict(toy.model3, newdata = data.frame(x.new))
toy.roc = roc(y.new, toy.pred)
plot(toy.roc, legacy.axes = T, print.auc = T)
summary(toy.model3)
```

In this case, we have AUC = 0.5 exactly and the ROC looks like a straight line. In this case, the standard error of the estimated coefficient for x is reasonable.

## 5.5

We now know that $n_iy_i\sim Binomial(n_i, \pi_i)$ and $\mu_i = E(y_i)=\pi_i$.
In addition, since $\pi_i = F(\sum_j \beta_{j}x_{ij})$, we also have $n_i = F^{-1}(\pi_i)$ (as $F$ is absolutely continuous).Another equation is $n_i = \sum_j \beta_j x_{ij}$.

According to the likelihood equation, we have: 
$$\begin{split}
\frac{\partial L(\mathbf{\beta})}{\partial \beta_j} &= \sum_{i=1}^{N} \frac{n_i(y_i-\mu_i)}{var(y_i)}\cdot  \frac{\partial \mu_i}{\partial n_i}\\ &= \sum_{i=1}^{N} \frac{n_i(y_i-\mu_i)}{var(y_i)}\cdot f(n_i)\space [f(\cdot) \space is \space the \space pdf \space corresponding \space to \space F(\cdot)] \\
\end{split}$$

Therefore, we obtain:
$$w_i = (\frac{\partial \mu_i}{\partial n_i})^2/var(y_i) = f^2(n_i)/var(y_i)$$
Then, let $W$ be the diagonal matrix with $w_i$ as the main diagonal elements. 
Hence the information matrix for MLE $\hat {\boldsymbol{\beta}}$ $J=X'WX$ and $var(\hat{\boldsymbol{\beta}})=J^{-1}$.

## 5.9

Use conditional logistic regression to test $H_0: \beta_1 = 0$ against $H_1: \beta_1<0$ for the toy example in Section 5.4.2.
According to the textbook page #175-176, we could know that,,
The p-value is $1-P(s_1\ge t \space | \space s_1+s_2)$, for observed value $t$ for $s_1$.
```{r clogit}
library(survival)
# data import
x = c(1, 2, 3, 4, 5, 6)
y = c(1, 1, 1, 0, 0, 0)
clogistic = clogit(y~x)
summary(clogistic)
```


## 5.14
Assuming $\pi_1 = \pi_2 = \cdots = \pi_N = \pi$, then the log likelihood would be
$$L(\pi)=\sum_{i=1}^{N}y_ilog(\pi)+(n_i-y_i)log(1-\pi)$$
Take the first derivative, we can get $$L'(\pi)=\frac{\sum{y_i}}{\pi}-\frac{\sum{n_i-y_i}}{1-\pi}$$
Set it equal to 0, we can get $$\hat{\pi}=(\sum{y_i})/(\sum{n_i})$$
And the second derivative of $L(\pi)$ also confirms that $\hat{\pi}$ maximizes the likelihood function.
Then the Pearson statistic for ungrouped data (when $ n_i=1$) is:
$$\begin{split}
\chi^2 &= \sum{\frac{(observed-fitted)^2}{fitted}} \\
&= \sum_{i=1}^{N} \sum_{j=1}^{n_i} \frac{(y_{ij}-\hat{\pi})^2}{\hat{\pi}} + \frac{[1-y_{ij}-(1-\hat{\pi})]^2}{1-\hat{\pi}} \\
&= \sum_{i=1}^{N} \sum_{j=1}^{n_i} \frac{(y_{ij}-\hat{\pi})^2}{\hat{\pi}(1-\hat{\pi})}\\
&= \frac{N\hat{\pi}(1-\hat{\pi})}{\hat{\pi}(1-\hat{\pi})} = N
\end{split}$$

Since the Pearson statistic $\chi^2=N$, the statistic is not informative for us to test the goodness-of-fit of the null model.

## 5.15
The log likelihood is $\sum_i[y_ilog\pi_i+(1-y_i)log(1-\pi_i)]$. For the saturated model, we have $\hat{\pi_i}=y_i$ and the value of the log likelihood of saturated model equals 0 (because $y_i$ can only take value of 0 and 1).
$$\begin{split}
D(y; \boldsymbol{\hat{\mu}})&=-2\sum observed\times log(observed/fitted) \\
&=-2 (\sum_i y_{ij}log(\frac{y_i}{\hat{\pi_i}})+\sum_i (1-y_i)log(\frac{1-y_i}{1-\hat{\pi}})) \\
&=-2\sum_i [y_ilog(\frac{\hat{\pi_i}}{1-\hat{\pi_i}})+log(1-\hat{\pi_i})] \\
&=-2 \sum_i [y_i(\hat{\beta_0} + \hat{\beta_1}x_i) + log(1-\hat{\pi_i})]
\end{split}$$
From 5.14, we could know that $\sum_i y_i = \sum_i \hat{\pi_i}$ and so $\sum_i x_i=\sum_i x_i\hat{\pi_i}$.
So the deviance would be:
$$\begin{split}
D&=-2[\hat{\beta_0}\sum_i\hat{\pi_i}+\hat{\beta_1}\sum_ix_i\hat{\pi_i}+\sum_ilog(1-\hat{\pi_i})] \\
&=-2[\sum_i\hat{\pi_i}(\hat{\beta_0}+\hat{\beta_1}x_i)+\sum_ilog(1-\hat{\pi_i})] \\
&=-2\sum_i\hat{\pi_i}log(\frac{\hat{\pi_i}}{1-\hat{\pi_i}})-2\sum_ilog(1-\hat{\pi_i})
\end{split}$$
Therefore, the deviance only depends on $\hat{\pi_i}$, and it is uninformative for checking model fit.

## 5.16

#### (a)

If we treat the data as N binomial observations and let $s_i=\sum_{j=1}^{n_i}$, the kernel of the log likelihood is:
$$L(\mathbf{\pi})=\sum_{i=1}^{N}=s_i log(\pi_i)+ (n_i - s_i)log(1-\pi_i)$$
If we treat the data as n Bernoulli observations, the kernel of the log likelihood is 
$$\begin{split}
L(\mathbf{\pi})&=\sum_{i=1}^{N} \sum_{j=1}^{n_i} y_{ij} log(\pi_i) + (1 -y_{ij})log(1-\pi_i)\\
&=\sum_{i=1}^{N} s_i log(\pi_i) + (n_i-s_i)log(1-\pi_i)
\end{split}$$

#### (b)

For saturated model, explain why the likelihood function is different for these two data forms.

* If we treat the data as N Binomial observations, there are N parameters ($\pi_1, \cdots,\pi_N$).
* If we treat the data as n Bernoulli observations, there are n parameters ($\pi_{11},\cdots,\pi_{Nn_i}$).

#### (c)

Explain why the difference between deviances for two unsaturated models does not depend on the form of data entry.

They do not depend on the form of data entry because when subtracting, the log likelihood of saturated models cancel out, so the result only depends on the log likelihoods of unsaturated models. We aready know from (a) part that the log likelihood of unsaturated models do not depend on the form of data entry.

## 5.17

Create a data file in two ways.

#### (a)
```{r create data}
# Ungrouped data
x.ungroup = c(rep(0, 4), rep(1, 4), rep(2, 4))
y.ungroup = c(1, 0, 0, 0, 1, 1, 0, 0, rep(1, 4))

# Grouped data
x.group = c(0, 1, 2)
n.trials = c(4, 4, 4)
n.successes = c(1, 2, 4)
resp = cbind(n.successes, n.trials - n.successes)

# Fit models
model.ungrouped = glm(y.ungroup ~ x.ungroup, family = "binomial")
summary(model.ungrouped)

model.grouped = glm(resp~x.group, family = "binomial")
summary(model.grouped)
```

From the summaries, we could know that:

* For ungrouped data, the deviance for $M_0$ us 16.3 and the deviance for $M_1$ is 11.0.
* For grouped data, the deviance for $M_0$ us 6.3 and the deviance for $M_1$ is 1.0.

The saturated model in the ungrouped case has 12 parameters (df = 11) and the saturated model in the grouped case only has three parameters.


#### (b)

The differences between the deviances are the same. 
16.3 - 11.0 = 6.3 - 1.0 = 5.3

Explanation: the only difference between log likelihoods of these two data entry forms is the binomial coefficients. However, it cancels out when we do the subtraction. Therefore, the differences of deviances are the same.

## 5.19

Suppose $\pi_{ab} + \pi_{ba} =1$ and we are using the model 
$$log(\pi_{ab}/\pi_{ba})=\beta_a-\beta_b.$$
For $a<b$, let $N_{ab}$ denote the number of matches between teams $a$ and $b$, with team $a$ winning $n_{ab}$ times and team $b$ winning $n_{ba}$ times.

#### (a)

Find the log-likelihood, treating $n_{ab}$ as a binomial variate for $N_{ab}$ trials. Show that sufficient statistics are $\left\{n_{a+}\right\}$, so that "victory totals" determine the estimated ranking of teams.
Since $\pi_{ab} + \pi_{ba} =1$, we have $\pi_{ba} = 1 - \pi_{ab}$. Then the model could be re-written as:
$$log(\frac{\pi_{ab}}{1-\pi_{ab}})=\beta_a-\beta_b$$
The likelihood function would be:
$$\begin{split}
l &= {N_{ab}\choose n_{ab}}\cdot \pi_{ab}^{n_{ab}} \cdot (1- \pi_{ab})^{N_{ab}-n_{ab}} \\
&={N_{ab}\choose n_{ab}}\cdot(1-\pi_{ab})^{N_{ab}}\cdot [\frac{\pi_{ab}}{(1-\pi_{ab})}]^{n_{ab}}
\end{split}$$
So the first two terms are the function related to $n_{ab}$ and the later two terms are the function related to $\pi_{ab}$ and it interact with $\pi_{ab}$ only through $n_{ab}$.

#### (b)
