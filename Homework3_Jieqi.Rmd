---
title: "Homework 3"
author: "Jieqi Tu"
date: "3/30/2021"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## 5.3

#### (a)
```{r}

```

## 5.14
Assuming $\pi_1 = \pi_2 = \cdots = \pi_N = \pi$, then the log likelihood would be
$$L(\pi)=\sum_{i=1}^{N}y_ilog(\pi)+(n_i-y_i)log(1-\pi)$$
Take the first derivative, we can get $$L'(\pi)=\frac{\sum{y_i}}{\pi}-\frac{\sum{n_i-y_i}}{1-\pi}$$
Set it equal to 0, we can get $$\hat{\pi}=(\sum{y_i})/(\sum{n_i})$$
And the second derivative of $L(\pi)$ also confirms that $\hat{\pi}$ maximizes the likelihood function.
Then the Pearson statistic for ungrouped data (when $ n_i=1$) is:
$$\begin{split}
\chi^2 &= \sum{\frac{(observed-fitted)^2}{fitted}} \\
&= \sum_{i=1}^{N} \sum_{j=1}^{n_i} \frac{(y_{ij}-\hat{\pi})^2}{\hat{\pi}} + \frac{[1-y_{ij}-(1-\hat{\pi})]^2}{1-\hat{\pi}} \\
&= \sum_{i=1}^{N} \sum_{j=1}^{n_i} \frac{(y_{ij}-\hat{\pi})^2}{\hat{\pi}(1-\hat{\pi})}\\
&= \frac{N\hat{\pi}(1-\hat{\pi})}{\hat{\pi}(1-\hat{\pi})} = N

\end{split}$$

Since the Pearson statistic $\chi^2=N$, the statistic is not informative for us to test the goodness-of-fit of the null model.

## 5.15
The log likelihood is $\sum_i[y_ilog\pi_i+(1-y_i)log(1-\pi_i)]$. For the saturated model, we have $\hat{\pi_i}=y_i$ and the value of the log likehood of saturated model equals 0 (because $y_i$ can only take value of 0 and 1).
$$\begin{split}
D(y; \boldsymbol{\hat{\mu}})&=-2\sum observed\times log(observed/fitted) \\
&=-2 (\sum_i y_{ij}log(\frac{y_i}{\hat{\pi_i}})+\sum_i (1-y_i)log(\frac{1-y_i}{1-\hat{\pi}})) \\
&=-2\sum_i [y_ilog(\frac{\hat{\pi_i}}{1-\hat{\pi_i}})+log(1-\hat{\pi_i})] \\
&=-2 \sum_i [y_i(\hat{\beta_0} + \hat{\beta_1}x_i) + log(1-\hat{\pi_i})]
\end{split}$$
From 5.14, we could know that $\sum_i y_i = \sum_i \hat{\pi_i}$ and so $\sum_i x_i=\sum_i x_i\hat{\pi_i}$.
So the deviance would be:
$$\begin{split}
D&=-2[\hat{\beta_0}\sum_i\hat{\pi_i}+\hat{\beta_1}\sum_ix_i\hat{\pi_i}+\sum_ilog(1-\hat{\pi_i})] \\
&=-2[\sum_i\hat{\pi_i}(\hat{\beta_0}+\hat{\beta_1}x_i)+\sum_ilog(1-\hat{\pi_i})] \\
&=-2\sum_i\hat{\pi_i}log(\frac{\hat{\pi_i}}{1-\hat{\pi_i}})-2\sum_ilog(1-\hat{\pi_i})
\end{split}$$
Therefore, the deviance only depends on $\hat{\pi_i}$, and it is uninformative for checking model fit.