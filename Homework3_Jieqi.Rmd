---
title: "Homework 3"
author: "Jieqi Tu"
date: "3/30/2021"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ROCR)
library(pROC)
```

## 5.3

#### (a)
Construct the ROC curve for the toy example in Section 5.4.2. with complete separation.
```{r 5.3a}
# data import
x = c(1, 2, 3, 4, 5, 6)
y = c(1, 1, 1, 0, 0, 0)

# fit model
toy.model = glm(y~x, family = "binomial")
toy.pred = predict(toy.model, newdata = data.frame(x))
toy.roc = roc(y, toy.pred)
plot(toy.roc, legacy.axes = T, print.auc = T)
summary(toy.model)
```

From the result we could see that, when fitting the GLM, we have a warning message saying that fitted probabilities numerically 0 or 1 occurred. This implies complete/semi-complete separation occurred. 
We could see that the AUC = 1. Therefore, in this case, we have complete separation.The standard error of the estimated coefficient for x is very large.

#### (b)
Add two observations at x = 0.5, one with y = 1 and one with 0.
```{r 5.3b}
x = c(1, 2, 3, 3.5, 3.5, 4, 5, 6)
y = c(1, 1, 1, 1, 0, 0, 0, 0)

toy.model2 = glm(y~x, family = "binomial")
toy.pred = predict(toy.model2, newdata = data.frame(x))
toy.roc = roc(y, toy.pred)
plot(toy.roc, legacy.axes = T, print.auc = T)
summary(toy.model2)
```

In this case, we have AUC = 0.969.But we also have the warning message. So we are facing the semi-complete separation. The standard error for estimated coefficient for x is still very large.

Then we want to construct a toy data set with n = 8 and the area under the ROC curve equals 0.5.
```{r 5.3c}
# construct new dataset
x.new = c(1, 1, 2, 2, 3, 3, 4, 4)
y.new = c(1, 0, 1, 0, 1, 0, 1, 0)

# fit new model for new dataset
toy.model3 = glm(y.new~x.new, family = "binomial")
toy.pred = predict(toy.model3, newdata = data.frame(x.new))
toy.roc = roc(y.new, toy.pred)
plot(toy.roc, legacy.axes = T, print.auc = T)
summary(toy.model3)
```

In this case, we have AUC = 0.5 exactly and the ROC looks like a straight line. In this case, the standard error of the estimated coefficient for x is reasonable.


## 5.14
Assuming $\pi_1 = \pi_2 = \cdots = \pi_N = \pi$, then the log likelihood would be
$$L(\pi)=\sum_{i=1}^{N}y_ilog(\pi)+(n_i-y_i)log(1-\pi)$$
Take the first derivative, we can get $$L'(\pi)=\frac{\sum{y_i}}{\pi}-\frac{\sum{n_i-y_i}}{1-\pi}$$
Set it equal to 0, we can get $$\hat{\pi}=(\sum{y_i})/(\sum{n_i})$$
And the second derivative of $L(\pi)$ also confirms that $\hat{\pi}$ maximizes the likelihood function.
Then the Pearson statistic for ungrouped data (when $ n_i=1$) is:
$$\begin{split}
\chi^2 &= \sum{\frac{(observed-fitted)^2}{fitted}} \\
&= \sum_{i=1}^{N} \sum_{j=1}^{n_i} \frac{(y_{ij}-\hat{\pi})^2}{\hat{\pi}} + \frac{[1-y_{ij}-(1-\hat{\pi})]^2}{1-\hat{\pi}} \\
&= \sum_{i=1}^{N} \sum_{j=1}^{n_i} \frac{(y_{ij}-\hat{\pi})^2}{\hat{\pi}(1-\hat{\pi})}\\
&= \frac{N\hat{\pi}(1-\hat{\pi})}{\hat{\pi}(1-\hat{\pi})} = N
\end{split}$$

Since the Pearson statistic $\chi^2=N$, the statistic is not informative for us to test the goodness-of-fit of the null model.

## 5.15
The log likelihood is $\sum_i[y_ilog\pi_i+(1-y_i)log(1-\pi_i)]$. For the saturated model, we have $\hat{\pi_i}=y_i$ and the value of the log likehood of saturated model equals 0 (because $y_i$ can only take value of 0 and 1).
$$\begin{split}
D(y; \boldsymbol{\hat{\mu}})&=-2\sum observed\times log(observed/fitted) \\
&=-2 (\sum_i y_{ij}log(\frac{y_i}{\hat{\pi_i}})+\sum_i (1-y_i)log(\frac{1-y_i}{1-\hat{\pi}})) \\
&=-2\sum_i [y_ilog(\frac{\hat{\pi_i}}{1-\hat{\pi_i}})+log(1-\hat{\pi_i})] \\
&=-2 \sum_i [y_i(\hat{\beta_0} + \hat{\beta_1}x_i) + log(1-\hat{\pi_i})]
\end{split}$$
From 5.14, we could know that $\sum_i y_i = \sum_i \hat{\pi_i}$ and so $\sum_i x_i=\sum_i x_i\hat{\pi_i}$.
So the deviance would be:
$$\begin{split}
D&=-2[\hat{\beta_0}\sum_i\hat{\pi_i}+\hat{\beta_1}\sum_ix_i\hat{\pi_i}+\sum_ilog(1-\hat{\pi_i})] \\
&=-2[\sum_i\hat{\pi_i}(\hat{\beta_0}+\hat{\beta_1}x_i)+\sum_ilog(1-\hat{\pi_i})] \\
&=-2\sum_i\hat{\pi_i}log(\frac{\hat{\pi_i}}{1-\hat{\pi_i}})-2\sum_ilog(1-\hat{\pi_i})
\end{split}$$
Therefore, the deviance only depends on $\hat{\pi_i}$, and it is uninformative for checking model fit.